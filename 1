 A Python library is a collection of functions and methods that allow you to perform lots of actions without writing any code. 
 The libraries usually contain built-in modules providing different functionalities which you can use directly. 
 There are extensive libraries offering a broad range of facilities. 
 We have divided the Python data analysis libraries into three groups.
 The first group is called Scientific Computing Libraries. 
 Pandas offers data structure and tools for effective data manipulation and analysis. 
 It provides fast access to structured data. 
 The primary instrument of Pandas is a two dimensional table consisting of column and row labels, which are called a data frame. 
 It is designed to provide easy indexing functionality. The NumPy library uses arrays for its inputs and outputs. 
 It can be extended to objects for matrices, and with minor coding changes, developers can perform fast array processing. 
 SciPy includes functions for some advanced math problems as listed on this slide, as well as data visualization.
 Using data visualization methods is the best way to communicate with others, showing the meaningful results of analysis. 
 These libraries enable you to create graphs, charts, and maps. The Matplotlib lib package is the most well-known library for data visualization.
 It is great for making graphs and plots. The graphs are also highly customizable. Another high-level visualization library is Seaborn. 
 It is based on Matplotlib. It's very easy to generate various plots, such as heat maps, time series, and violin plots. 
 With machine learning algorithms, we're able to develop a model using our data set and obtain predictions. 
 The algorithmic libraries tackle some machine learning tasks from basic too complex. Here we introduce two packages. 
 The Scikit-learn library contains tools, for statistical modeling, including regression, classification, clustering, and so on. 
 This library is built on NumPy, SciPy, and Matplotlib. 
 Statsmodels is also a Python module that allows users to explore data, estimate statistical models, and perform statistical tests.

#Each line in a dataset is a row, and commas separate the values.
#To understand the data, you must analyze the attributes for each column of data.
#Python libraries are collections of functions and methods that facilitate various 
        functionalities without writing code from scratch and are categorized into Scientific Computing, Data Visualization, and Machine Learning Algorithms.
#Many data science libraries are interconnected; for instance, Scikit-learn is built on top of NumPy, SciPy, and Matplotlib.
#The data format and the file path are two key factors for reading data with Pandas.
#The read_CSV method in Pandas can read files in CSV format into a Pandas DataFrame.
#Pandas has unique data types like object, float, Int, and datetime.
#Use the dtype method to check each column’s data type; misclassified data types might need manual correction.
#Knowing the correct data types helps apply appropriate Python functions to specific columns.
#Using Statistical Summary with describe() provides count, mean, standard deviation, min, max, and quartile ranges for numerical columns.
#You can also use include='all' as an argument to get summaries for object-type columns.
#The statistical summary helps identify potential issues like outliers needing further attention.
#Using the info() Method gives an overview of the top and bottom 30 rows of the DataFrame, useful for quick visual inspection.
#Some statistical metrics may return "NaN," indicating missing values, and the program can’t calculate statistics for that specific data type.
#Python can connect to databases through specialized code, often written in Jupyter notebooks.
#SQL Application Programming Interfaces (APIs) and Python DB APIs (most often used) facilitate the interaction between Python and the DBMS.
#SQL APIs connect to DBMS with one or more API calls, build SQL statements as a text string,
        and use API calls to send SQL statements to the DBMS and retrieve results and statuses.
#DB-API, Python's standard for interacting with relational databases, 
        uses connection objects to establish and manage database connections and cursor objects to run queries and scroll through the results.
#Connection Object methods include the cursor(), commit(), rollback(), and close() commands.
#You can import the database module, use the Connect API to open a connection, and then create a cursor object to run queries and fetch results. 
#Remember to close the database connection to free up resources.

